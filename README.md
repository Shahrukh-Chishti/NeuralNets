# Neural Networks-Empirical Structural Behaviour
Neural Networks is large, complex and non-linear family of functions that are utilised in modern machine learning. Inspired from mammalian nervous system, they provide diverse generalization for most of ML models. A gateway to Artificial Intelligence, the Deep Neural Networks, focus of our work, can ape almost any function, see http://neuralnetworksanddeeplearning.com/chap4.html
Structurally, we define Deep Neural Networks as stacked layers of neurons which are connected in a feedforward fashion, from input to output, via non-linear activations. Deep feedforward layers provide necessary connecting transformations of features in almost every architecture.<pic><equations>
The layers could be represented with the number of neurons they contain <math array> and a list of neurons given activation define morphology of a network.
Inputs & Outputs range ......
Functionally, at every synaptic junction there is a non-linear activation function, which, on the expense of increased classificability kills invertibility. Feedforwarding the many-one behaviour provides great filtering, which essentially allow quick aping abilities, makes reverse estimation 
Further, the non-linear behaviour makes it smooth inference 
In this study, we just consider
